{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apprentissage supervisé : analyse discriminante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "1. [Nuage de points](#part1)\n",
    "1. [Analyse linéaire discriminante](#part2)\n",
    "1. [Analyse quadratique discriminante](#part3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nuage de points <a id=\"part1\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def covariance(sigma1=1., sigma2=1., theta=0.):\n",
    "    \"\"\"\n",
    "        Covariance matrix with eigenvalues sigma1 and sigma2, rotated by the angle theta.\n",
    "    \"\"\"\n",
    "    rotation = np.array([[np.cos(theta), -np.sin(theta)],\n",
    "                        [np.sin(theta), np.cos(theta)]])\n",
    "    cov = np.array([[sigma1, 0.],\n",
    "                   [0, sigma2]])\n",
    "    return rotation.dot(cov.dot(rotation.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Construire une matrice de covariance.\n",
    "    Quelles sont les valeurs de ses composantes.\n",
    "    <br>\n",
    "    Simuler un jeu de données gaussien à partir de cette covariance.\n",
    "<!-- <br> -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "# Define cov with the function covariance\n",
    "# Todo\n",
    "cov = covariance(\n",
    "    sigma1=1.,\n",
    "    sigma2=1.,\n",
    "    theta=0.\n",
    ")\n",
    "# End todo\n",
    "\n",
    "X = multivariate_normal.rvs(cov=cov, size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Afficher le jeu de données généré et sa moyenne.\n",
    "<!-- <br> -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "print(\"Covariance matrix:\")\n",
    "print(cov)\n",
    "\n",
    "# Empirical mean mu\n",
    "mu = np.mean(X, axis=0)\n",
    "\n",
    "# Plot the data with plt.scatter\n",
    "# Todo\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "\n",
    "# End todo\n",
    "plt.scatter(mu[0], mu[1], c='k', marker='o', s=100)\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Faire varier la matrice de covariance.\n",
    "<!-- <br> -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_new = covariance(\n",
    "    sigma1=4.,\n",
    "    sigma2=1.,\n",
    "    theta=np.pi / 4\n",
    ")\n",
    "# Empirical mean mu\n",
    "X_new = multivariate_normal.rvs(cov=cov_new, size=1000)\n",
    "mu_new = np.mean(X_new, axis=0)\n",
    "\n",
    "# Plot the data with plt.scatter\n",
    "# Todo\n",
    "plt.scatter(X_new[:, 0], X_new[:, 1])\n",
    "\n",
    "# End todo\n",
    "plt.scatter(mu_new[0], mu_new[1], c='k', marker='o', s=100)\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Charger le jeu de données `iris` et afficher le nombre de classes.    \n",
    "<!-- <br> -->\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris_dataset = load_iris()\n",
    "X = iris_dataset[\"data\"]\n",
    "y = iris_dataset[\"target\"]\n",
    "\n",
    "# Print the number of classes\n",
    "# Todo\n",
    "unique_values, count = np.unique(y, return_counts=True)\n",
    "\n",
    "answer=  f\"le nombre de classes est: {len(unique_values)}\"\n",
    "print(answer)\n",
    "\n",
    "iris_dataset[\"target_names\"]\n",
    "# End todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = pd.DataFrame(X, columns=iris_dataset[\"feature_names\"])\n",
    "X_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df = pd.DataFrame(y, columns=[\"target\"])\n",
    "target_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Afficher les deux premières classes en fonction des deux premières variables explicatives.\n",
    "    Peut-on considérer les classes gaussiennes ?\n",
    "<!-- <br> -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "# Define sub-arrays X_sub and y_sub\n",
    "# Todo\n",
    "mask = y != 2\n",
    "X_sub = X[mask]\n",
    "y_sub = y[mask]\n",
    "# End todo\n",
    "\n",
    "plt.scatter(X_sub[:, 0], X_sub[:, 1], c=y_sub, cmap='plasma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse linéaire discriminante <a id=\"part2\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_sample(mu=[0, 0], sigma1=1., sigma2=1., theta=0., n=50):\n",
    "    cov = covariance(sigma1, sigma2, theta)\n",
    "    x = multivariate_normal.rvs(mean=mu, cov=cov, size=n)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Générer un jeu de données à partir de la fonction précédente.  \n",
    "    Calculer sa moyenne et sa matrice de covariance empiriques.\n",
    "<!-- <br> -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "sample = gaussian_sample(\n",
    "    mu=[0, 0],\n",
    "    sigma1=1.,\n",
    "    sigma2=1.,\n",
    "    theta=0.,\n",
    "    n=100\n",
    ")\n",
    "sample_mean = np.mean(sample, axis=0)\n",
    "sample_covariance_matrix= np.cov(sample[:, 0], sample[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_covariance_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Créer puis afficher un jeu de données constitué de deux classes gaussiennes.    \n",
    "<!-- <br> -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "# The two datasets X1 and X2 with different means\n",
    "# Todo\n",
    "n_samples=1000\n",
    "\n",
    "mu1_theoretical = [0, 0]\n",
    "X1 = gaussian_sample(\n",
    "    mu=mu1_theoretical,\n",
    "    n=n_samples\n",
    ")\n",
    "mu2_theoretical = [1, 1]\n",
    "X2 = gaussian_sample(\n",
    "    mu=mu2_theoretical,\n",
    "    n=n_samples\n",
    ")\n",
    "\n",
    "# End todo\n",
    "\n",
    "X = np.r_[X1, X2]\n",
    "y = np.r_[np.ones(X1.shape[0]), -np.ones(X2.shape[0])]\n",
    "\n",
    "# Todo\n",
    "\n",
    "# End todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_frontiere(clfs, data=None, data_labels=None, label=None, num=500, figure=True):\n",
    "    \"\"\"\n",
    "        Plot the frontiere fun(x)=0 of the classifier clf within the same range as the one\n",
    "        of the data.\n",
    "        Input:\n",
    "            clfs: classifier or list of classifiers\n",
    "            data: input data (X)\n",
    "            data_labels: data labels (y)\n",
    "            label: classifier labels as a list\n",
    "            num: discretization parameter\n",
    "            figure: create a new figure\n",
    "    \"\"\"\n",
    "    if not hasattr(clfs, '__iter__'):\n",
    "        clfs = [clfs]\n",
    "    if label is not None and not hasattr(label, '__iter__'):\n",
    "        label = [label]\n",
    "        \n",
    "    xmin, ymin = data.min(axis=0)\n",
    "    xmax, ymax = data.max(axis=0)\n",
    "    x, y = np.meshgrid(np.linspace(xmin, xmax, num), np.linspace(ymin, ymax))\n",
    "    \n",
    "    if figure:\n",
    "        plt.figure(figsize=(7, 7))\n",
    "#     plt.scatter(*data.T, c=data_labels, cmap='plasma')\n",
    "    for icl, cl in enumerate(np.unique(data_labels)):\n",
    "        plt.scatter(*data[data_labels==cl].T, label=f'Class {cl}')\n",
    "        \n",
    "    for i, clf in enumerate(clfs):\n",
    "        z = clf.decision_function(np.c_[x.ravel(), y.ravel()]).reshape(x.shape)\n",
    "        cs = plt.contour(x, y, z, [0], colors='r')\n",
    "        if label is not None:\n",
    "            cs.levels = [label[i]]\n",
    "            plt.gca().clabel(cs)\n",
    "    if figure:\n",
    "        plt.axis('image')\n",
    "    minx, miny = data[:, 0].min(), data[:, 1].min()\n",
    "    diffx, diffy = data[:, 0].max() - minx, data[:, 1].max() - miny\n",
    "    plt.axis([minx - 0.1*diffx, minx + 1.1*diffx, miny - 0.1*diffy, miny + 1.1*diffy])\n",
    "    plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Afficher la frontière obtenue par l'analyse linéaire discriminante ainsi que le segment défini par les moyennes des deux classes.\n",
    "<!-- <br> -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# Linear discriminant analysis\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "# Fit the model\n",
    "# Todo\n",
    "lda.fit(X, y)\n",
    "# End todo\n",
    "\n",
    "print(\"LDA parameters:\")\n",
    "print(lda.coef_, lda.intercept_)\n",
    "\n",
    "# Means mu1 and mu2 for the two classes\n",
    "# Todo\n",
    "mu1 = np.mean(X1, axis=0)\n",
    "mu2 = np.mean(X2, axis=0)\n",
    "\n",
    "# End todo\n",
    "\n",
    "plot_frontiere(lda, X, y)\n",
    "plt.plot([mu1[0], mu2[0]], [mu1[1], mu2[1]], 'ko-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse quadratique discriminante <a id=\"part3\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Compléter le code suivant pour comparer analyses discriminantes linéaire et quadratique dans diverses situations.    \n",
    "<!-- <br> -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Answer\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "\n",
    "# Gassian parameters\n",
    "mu1 = mu = [0, 0]\n",
    "mu2 = [5, 3]\n",
    "\n",
    "plt.figure(figsize=(10, 20))\n",
    "for (p1, p2) in [((1, 1, 0), ) * 2,\n",
    "                  ((1, 5, 0), ) * 2,\n",
    "                  ((1, 5, np.pi/6), ) * 2,\n",
    "                  ((1, 5, 0), (5, 1, 0)),\n",
    "                  ((1, 5, 0), (5, 1, np.pi/3))]:\n",
    "    # Dataset\n",
    "    X1 = gaussian_sample(mu1, *p1)\n",
    "    X2 = gaussian_sample(mu2, *p2)\n",
    "    X = np.r_[X1, X2]\n",
    "    Y = np.r_[np.ones(X1.shape[0]), -np.ones(X2.shape[0])]\n",
    "    \n",
    "    # Discriminant analysis\n",
    "    # Todo\n",
    "    qda.fit(X, Y)\n",
    "\n",
    "    # End todo\n",
    "    \n",
    "    # Class means\n",
    "    # Todo\n",
    "    mu1_empirical = np.mean(X1, axis=0)\n",
    "    mu2_empirical = np.mean(X2, axis=0)\n",
    "\n",
    "    # End todo\n",
    "    \n",
    "    # Plot frontieres and class means\n",
    "    # Todo\n",
    "\n",
    "    plot_frontiere(qda, X, Y)\n",
    "    plt.plot([mu1_empirical[0], mu2_empirical[0]], [mu1_empirical[1], mu2_empirical[1]], 'ko-')\n",
    "\n",
    "    # End todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
