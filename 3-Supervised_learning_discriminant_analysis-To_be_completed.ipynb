{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Apprentissage supervis\u00e9 : analyse discriminante"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Table of contents\n", "1. [Nuage de points](#part1)\n", "1. [Analyse lin\u00e9aire discriminante](#part2)\n", "1. [Analyse quadratique discriminante](#part3)\n"]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": ["import warnings\n", "warnings.filterwarnings('ignore')\n", "\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "%matplotlib inline\n", "sns.set()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Nuage de points <a id=\"part1\"></a>\n"]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": ["def covariance(sigma1=1., sigma2=1., theta=0.):\n", "    \"\"\"\n", "        Covariance matrix with eigenvalues sigma1 and sigma2, rotated by the angle theta.\n", "    \"\"\"\n", "    rotation = np.array([[np.cos(theta), -np.sin(theta)],\n", "                        [np.sin(theta), np.cos(theta)]])\n", "    cov = np.array([[sigma1, 0.],\n", "                   [0, sigma2]])\n", "    return rotation.dot(cov.dot(rotation.T))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-info\">\n", "    Construire une matrice de covariance.\n", "    Quelles sont les valeurs de ses composantes.\n", "    <br>\n", "    Simuler un jeu de donn\u00e9es gaussien \u00e0 partir de cette covariance.\n", "<!-- <br> -->\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "source": ["# Answer\n", "from scipy.stats import multivariate_normal\n", "\n", "# Define cov with the function covariance\n", "# Todo\n", "\n", "# End todo\n", "\n", "X = multivariate_normal.rvs(cov=cov, size=100)"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-info\">\n", "    Afficher le jeu de donn\u00e9es g\u00e9n\u00e9r\u00e9 et sa moyenne.\n", "<!-- <br> -->\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "source": ["# Answer\n", "print(\"Covariance matrix:\")\n", "print(cov)\n", "\n", "# Empirical mean mu\n", "# Todo\n", "\n", "# End todo\n", "\n", "# Plot the data with plt.scatter\n", "# Todo\n", "\n", "# End todo\n", "plt.scatter(mu[0], mu[1], c='k', marker='o', s=100)\n", "plt.axis('equal');"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-info\">\n", "    Faire varier la matrice de covariance.\n", "<!-- <br> -->\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "source": ["# Answer"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-info\">\n", "\n", "Charger le jeu de donn\u00e9es `iris` et afficher le nombre de classes.    \n", "<!-- <br> -->\n", "\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "source": ["# Answer\n", "from sklearn.datasets import load_iris\n", "\n", "X, y = load_iris(return_X_y=True)\n", "\n", "# Print the number of classes\n", "# Todo\n", "\n", "# End todo"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-info\">\n", "    Afficher les deux premi\u00e8res classes en fonction des deux premi\u00e8res variables explicatives.\n", "    Peut-on consid\u00e9rer les classes gaussiennes ?\n", "<!-- <br> -->\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "source": ["# Answer\n", "# Define sub-arrays X_sub and y_sub\n", "# Todo\n", "\n", "# End todo\n", "\n", "plt.scatter(X_sub[:, 0], X_sub[:, 1], c=y_sub, cmap='plasma')"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["# Analyse lin\u00e9aire discriminante <a id=\"part2\"></a>\n"]}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [], "source": ["def gaussian_sample(mu=[0, 0], sigma1=1., sigma2=1., theta=0., n=50):\n", "    cov = covariance(sigma1, sigma2, theta)\n", "    x = multivariate_normal.rvs(mean=mu, cov=cov, size=n)\n", "    return x"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-info\">\n", "    G\u00e9n\u00e9rer un jeu de donn\u00e9es \u00e0 partir de la fonction pr\u00e9c\u00e9dente.  \n", "    Calculer sa moyenne et sa matrice de covariance empiriques.\n", "<!-- <br> -->\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "source": ["# Answer"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-info\">\n", "Cr\u00e9er puis afficher un jeu de donn\u00e9es constitu\u00e9 de deux classes gaussiennes.    \n", "<!-- <br> -->\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "source": ["# Answer\n", "# The two datasets X1 and X2 with different means\n", "# Todo\n", "\n", "# End todo\n", "\n", "X = np.r_[X1, X2]\n", "y = np.r_[np.ones(X1.shape[0]), -np.ones(X2.shape[0])]\n", "\n", "# Todo\n", "\n", "# End todo"], "outputs": []}, {"cell_type": "code", "execution_count": 17, "metadata": {}, "outputs": [], "source": ["def plot_frontiere(clfs, data=None, data_labels=None, label=None, num=500, figure=True):\n", "    \"\"\"\n", "        Plot the frontiere fun(x)=0 of the classifier clf within the same range as the one\n", "        of the data.\n", "        Input:\n", "            clfs: classifier or list of classifiers\n", "            data: input data (X)\n", "            data_labels: data labels (y)\n", "            label: classifier labels as a list\n", "            num: discretization parameter\n", "            figure: create a new figure\n", "    \"\"\"\n", "    if not hasattr(clfs, '__iter__'):\n", "        clfs = [clfs]\n", "    if label is not None and not hasattr(label, '__iter__'):\n", "        label = [label]\n", "        \n", "    xmin, ymin = data.min(axis=0)\n", "    xmax, ymax = data.max(axis=0)\n", "    x, y = np.meshgrid(np.linspace(xmin, xmax, num), np.linspace(ymin, ymax))\n", "    \n", "    if figure:\n", "        plt.figure(figsize=(7, 7))\n", "#     plt.scatter(*data.T, c=data_labels, cmap='plasma')\n", "    for icl, cl in enumerate(np.unique(data_labels)):\n", "        plt.scatter(*data[data_labels==cl].T, label=f'Class {cl}')\n", "        \n", "    for i, clf in enumerate(clfs):\n", "        z = clf.decision_function(np.c_[x.ravel(), y.ravel()]).reshape(x.shape)\n", "        cs = plt.contour(x, y, z, [0], colors='r')\n", "        if label is not None:\n", "            cs.levels = [label[i]]\n", "            plt.gca().clabel(cs)\n", "    if figure:\n", "        plt.axis('image')\n", "    minx, miny = data[:, 0].min(), data[:, 1].min()\n", "    diffx, diffy = data[:, 0].max() - minx, data[:, 1].max() - miny\n", "    plt.axis([minx - 0.1*diffx, minx + 1.1*diffx, miny - 0.1*diffy, miny + 1.1*diffy])\n", "    plt.legend(loc=\"best\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-info\">\n", "Afficher la fronti\u00e8re obtenue par l'analyse lin\u00e9aire discriminante ainsi que le segment d\u00e9fini par les moyennes des deux classes.\n", "<!-- <br> -->\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "source": ["# Answer\n", "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n", "\n", "# Linear discriminant analysis\n", "lda = LinearDiscriminantAnalysis()\n", "# Fit the model\n", "# Todo\n", "\n", "# End todo\n", "\n", "print(\"LDA parameters:\")\n", "print(lda.coef_, lda.intercept_)\n", "\n", "# Means mu1 and mu2 for the two classes\n", "# Todo\n", "\n", "# End todo\n", "\n", "plot_frontiere(lda, X, y)\n", "plt.plot([mu1[0], mu2[0]], [mu1[1], mu2[1]], 'ko-')"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["# Analyse quadratique discriminante <a id=\"part3\"></a>\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-info\">\n", "Compl\u00e9ter le code suivant pour comparer analyses discriminantes lin\u00e9aire et quadratique dans diverses situations.    \n", "<!-- <br> -->\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "source": ["# Answer\n", "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n", "\n", "qda = QuadraticDiscriminantAnalysis()\n", "\n", "# Gassian parameters\n", "mu1 = mu = [0, 0]\n", "mu2 = [5, 3]\n", "\n", "plt.figure(figsize=(10, 20))\n", "for (p1, p2) in [((1, 1, 0), ) * 2,\n", "                  ((1, 5, 0), ) * 2,\n", "                  ((1, 5, np.pi/6), ) * 2,\n", "                  ((1, 5, 0), (5, 1, 0)),\n", "                  ((1, 5, 0), (5, 1, np.pi/3))]:\n", "    # Dataset\n", "    X1 = gaussian_sample(mu1, *p1)\n", "    X2 = gaussian_sample(mu2, *p2)\n", "    X = np.r_[X1, X2]\n", "    Y = np.r_[np.ones(X1.shape[0]), -np.ones(X2.shape[0])]\n", "    \n", "    # Discriminant analysis\n", "    # Todo\n", "\n", "    # End todo\n", "    \n", "    # Class means\n", "    # Todo\n", "\n", "    # End todo\n", "    \n", "    # Plot frontieres and class means\n", "    # Todo\n", "\n", "    # End todo"], "outputs": []}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.12.3"}}, "nbformat": 4, "nbformat_minor": 4}